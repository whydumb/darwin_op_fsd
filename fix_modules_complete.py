"""
IRO AI ëª¨ë“ˆ ì™„ì „ ìƒì„± ë° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys

print("ğŸ”§ IRO AI ëª¨ë“ˆ ë¬¸ì œ ì™„ì „ í•´ê²°")
print("=" * 60)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = os.path.dirname(os.path.abspath(__file__))
backend_path = os.path.join(project_root, 'backend')

print(f"ğŸ“‚ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
print(f"ğŸ“‚ ë°±ì—”ë“œ ê²½ë¡œ: {backend_path}")

def create_file(filepath, content):
    """íŒŒì¼ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"   âœ… ìƒì„±: {filepath}")

# 1ë‹¨ê³„: í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
print("\nğŸ“ 1ë‹¨ê³„: ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±...")
directories = [
    'backend/neural_network',
    'backend/knowledge_base', 
    'backend/api_integration',
    'backend/api',
    'backend/utils',
    'data/models',
    'data/knowledge',
    'data/logs'
]

for directory in directories:
    os.makedirs(directory, exist_ok=True)
    print(f"   âœ… {directory}")

# 2ë‹¨ê³„: __init__.py íŒŒì¼ë“¤ ìƒì„±
print("\nğŸ“ 2ë‹¨ê³„: íŒ¨í‚¤ì§€ ì´ˆê¸°í™” íŒŒì¼ ìƒì„±...")
init_files = [
    'backend/__init__.py',
    'backend/neural_network/__init__.py',
    'backend/knowledge_base/__init__.py',
    'backend/api_integration/__init__.py',
    'backend/api/__init__.py',
    'backend/utils/__init__.py'
]

for init_file in init_files:
    package_name = os.path.dirname(init_file).replace('/', '.').replace('\\', '.')
    content = f'"""{package_name} íŒ¨í‚¤ì§€"""\n'
    create_file(init_file, content)

# 3ë‹¨ê³„: í•µì‹¬ ëª¨ë“ˆ íŒŒì¼ ìƒì„±
print("\nğŸ§  3ë‹¨ê³„: ì‹ ê²½ë§ ëª¨ë“ˆ ìƒì„±...")

growing_network_code = '''"""
ìê°€ ì„±ì¥í˜• ì‹ ê²½ë§ - ì‹¤ì œ í•™ìŠµí•˜ê³  ì„±ì¥í•˜ëŠ” AI
"""

import numpy as np
import pickle
import os
from datetime import datetime

class SelfGrowingNeuralNetwork:
    def __init__(self, input_size=10, hidden_size=8, output_size=3, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # Xavier ì´ˆê¸°í™”
        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)
        self.b2 = np.zeros((1, output_size))
        
        # í•™ìŠµ ì´ë ¥
        self.training_history = {
            'loss': [], 'accuracy': [], 'epochs': 0,
            'growth_events': [], 'total_conversations': 0
        }
        
        print(f"ğŸ§  ì‹ ê²½ë§ ì´ˆê¸°í™”: {hidden_size}ê°œ ë‰´ëŸ°")
    
    def relu(self, x):
        """ReLU í™œì„±í™” í•¨ìˆ˜"""
        return np.maximum(0, x)
    
    def softmax(self, x):
        """Softmax í™œì„±í™” í•¨ìˆ˜"""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward(self, X):
        """ìˆœì „íŒŒ"""
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.relu(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.softmax(self.z2)
        return self.a2
    
    def backward(self, X, y_true, y_pred):
        """ì—­ì „íŒŒ - ì‹¤ì œ í•™ìŠµ"""
        m = X.shape[0]
        
        # ì¶œë ¥ì¸µ ê·¸ë˜ë””ì–¸íŠ¸
        dz2 = y_pred - y_true
        dW2 = np.dot(self.a1.T, dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m
        
        # ì€ë‹‰ì¸µ ê·¸ë˜ë””ì–¸íŠ¸
        da1 = np.dot(dz2, self.W2.T)
        dz1 = da1 * (self.z1 > 0)  # ReLU ë¯¸ë¶„
        dW1 = np.dot(X.T, dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m
        
        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
    
    def train(self, X, y, epochs=50, verbose=True):
        """ì‹ ê²½ë§ í•™ìŠµ"""
        if verbose:
            print(f"\\nğŸ“ í•™ìŠµ ì‹œì‘: {X.shape[0]}ê°œ ìƒ˜í”Œ, {epochs} ì—í¬í¬")
        
        for epoch in range(epochs):
            output = self.forward(X)
            loss = -np.sum(y * np.log(np.clip(output, 1e-15, 1-1e-15))) / X.shape[0]
            self.backward(X, y, output)
            
            accuracy = np.mean(np.argmax(output, axis=1) == np.argmax(y, axis=1))
            self.training_history['loss'].append(loss)
            self.training_history['accuracy'].append(accuracy)
            
            if verbose and (epoch + 1) % 10 == 0:
                print(f"   ì—í¬í¬ {epoch+1}/{epochs} - ì†ì‹¤: {loss:.4f}, ì •í™•ë„: {accuracy*100:.1f}%")
        
        self.training_history['epochs'] += epochs
        return accuracy
    
    def grow_network(self, new_neurons=2):
        """ì‹ ê²½ë§ í™•ì¥ - ë‡Œ ìš©ëŸ‰ ì¦ê°€"""
        print(f"\\nğŸŒ± ì‹ ê²½ë§ ì„±ì¥: {self.hidden_size} â†’ {self.hidden_size + new_neurons}ê°œ ë‰´ëŸ°")
        
        old_size = self.hidden_size
        self.hidden_size += new_neurons
        
        # ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ìƒì„±
        new_W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2.0 / self.input_size)
        new_b1 = np.zeros((1, self.hidden_size))
        new_W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2.0 / self.hidden_size)
        
        # ê¸°ì¡´ ì§€ì‹ ë³´ì¡´
        new_W1[:, :old_size] = self.W1
        new_b1[:, :old_size] = self.b1
        new_W2[:old_size, :] = self.W2
        
        # ê°€ì¤‘ì¹˜ êµì²´
        self.W1, self.b1, self.W2 = new_W1, new_b1, new_W2
        
        # ì„±ì¥ ì´ë²¤íŠ¸ ê¸°ë¡
        self.training_history['growth_events'].append({
            'timestamp': datetime.now().isoformat(),
            'old_size': old_size, 'new_size': self.hidden_size
        })
        
        print("âœ… ì‹ ê²½ë§ í™•ì¥ ì™„ë£Œ! ğŸ§ âœ¨")
    
    def should_grow(self, accuracy, data_count):
        """ìë™ ì„±ì¥ íŒë‹¨"""
        if self.hidden_size >= 30:
            return False, "ìµœëŒ€ í¬ê¸° ë„ë‹¬"
        if accuracy < 0.7:
            return True, f"ë‚®ì€ ì •í™•ë„ ({accuracy*100:.1f}%)"
        if data_count > 20 and self.hidden_size < 15:
            return True, f"ì¶©ë¶„í•œ ë°ì´í„° ({data_count}ê°œ)"
        return False, "í˜„ì¬ í¬ê¸°ë¡œ ì¶©ë¶„"
    
    def predict(self, X):
        """ì˜ˆì¸¡"""
        return np.argmax(self.forward(X), axis=1)
    
    def get_brain_status(self):
        """ë‡Œ ìƒíƒœ ì •ë³´"""
        return {
            'neurons': self.hidden_size,
            'total_parameters': (self.input_size * self.hidden_size + 
                               self.hidden_size * self.output_size + 
                               self.hidden_size + self.output_size),
            'epochs_trained': self.training_history['epochs'],
            'growth_events': len(self.training_history['growth_events']),
            'conversations': self.training_history['total_conversations']
        }
    
    def save(self, filepath):
        """ëª¨ë¸ ì €ì¥"""
        data = {
            'weights': {'W1': self.W1, 'b1': self.b1, 'W2': self.W2, 'b2': self.b2},
            'config': {'input_size': self.input_size, 'hidden_size': self.hidden_size,
                      'output_size': self.output_size, 'learning_rate': self.learning_rate},
            'history': self.training_history
        }
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
        print(f"ğŸ’¾ ì €ì¥: {filepath}")
    
    @classmethod
    def load(cls, filepath):
        """ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(filepath):
            return None
        try:
            with open(filepath, 'rb') as f:
                data = pickle.load(f)
            config = data['config']
            nn = cls(**config)
            weights = data['weights']
            nn.W1, nn.b1, nn.W2, nn.b2 = weights['W1'], weights['b1'], weights['W2'], weights['b2']
            nn.training_history = data['history']
            print(f"ğŸ“‚ ë¡œë“œ: {nn.hidden_size}ê°œ ë‰´ëŸ°")
            return nn
        except Exception as e:
            print(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
'''

create_file('backend/neural_network/growing_network.py', growing_network_code)

print("\nğŸ” 4ë‹¨ê³„: íŠ¹ì§• ì¶”ì¶œê¸° ìƒì„±...")

feature_extractor_code = '''"""
IRO ë¡œë´‡ ëŒ€íšŒ íŠ¹í™” íŠ¹ì§• ì¶”ì¶œê¸°
"""

import numpy as np

class IRORobotFeatureExtractor:
    def __init__(self):
        # IRO ê´€ë ¨ í‚¤ì›Œë“œ ì‚¬ì „
        self.tech_keywords = ['ì•„ë‘ì´ë…¸', 'arduino', 'ì„¼ì„œ', 'sensor', 'ëª¨í„°', 'motor',
                             'ì½”ë”©', 'í”„ë¡œê·¸ë˜ë°', 'c++', 'cì–¸ì–´', 'ë¼ì¸íŠ¸ë ˆì´ì‹±']
        self.iro_keywords = ['iro', 'ë¡œë´‡', 'robot', 'ì˜¬ë¦¼í”¼ì•„ë“œ', 'ëŒ€íšŒ', 'ìš°ì£¼', 'ë¯¸ì…˜']
        self.creative_keywords = ['ì•„ì´ë””ì–´', 'ë””ìì¸', 'ì°½ì˜', 'ì„¤ê³„', 'ê°œë°œ']
        
        print("ğŸ” IRO íŠ¹í™” íŠ¹ì§• ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_features(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ 10ì°¨ì› íŠ¹ì§• ë²¡í„°ë¡œ ë³€í™˜"""
        if not text or not text.strip():
            return np.zeros((1, 10))
        
        features = []
        text_lower = text.lower()
        words = text.split()
        word_count = len(words)
        
        # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ (ì •ê·œí™”)
        features.append(min(len(text) / 100.0, 1.0))
        
        # 2. ë‹¨ì–´ ìˆ˜ (ì •ê·œí™”)
        features.append(min(word_count / 30.0, 1.0))
        
        # 3. ì§ˆë¬¸ í‘œí˜„
        question_indicators = ['ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ', '?', 'ë°©ë²•']
        has_question = any(indicator in text for indicator in question_indicators)
        features.append(1.0 if has_question else 0.0)
        
        # 4. ê¸°ìˆ  í‚¤ì›Œë“œ ë°€ë„
        tech_count = sum(1 for keyword in self.tech_keywords if keyword in text_lower)
        features.append(min(tech_count / 3.0, 1.0))
        
        # 5. IRO ëŒ€íšŒ ê´€ë ¨ë„
        iro_count = sum(1 for keyword in self.iro_keywords if keyword in text_lower)
        features.append(min(iro_count / 2.0, 1.0))
        
        # 6. ì°½ì˜ì  í‘œí˜„
        creative_count = sum(1 for keyword in self.creative_keywords if keyword in text_lower)
        features.append(min(creative_count / 2.0, 1.0))
        
        # 7. ëª…ë ¹/ìš”ì²­ í‘œí˜„
        command_keywords = ['í•´ì¤˜', 'ì•Œë ¤ì¤˜', 'ì„¤ëª…í•´', 'ë„ì™€ì¤˜']
        is_command = any(keyword in text for keyword in command_keywords)
        features.append(1.0 if is_command else 0.0)
        
        # 8. ê°ì • í‘œí˜„
        emotion_indicators = ['!', 'ã…‹', 'ã…', 'ì¢‹ì•„', 'ê°ì‚¬']
        emotion_count = sum(1 for indicator in emotion_indicators if indicator in text)
        features.append(min(emotion_count / 3.0, 1.0))
        
        # 9. ìˆ«ì í¬í•¨ ì—¬ë¶€
        has_numbers = any(char.isdigit() for char in text)
        features.append(1.0 if has_numbers else 0.0)
        
        # 10. ë¬¸ì¥ ë³µì¡ë„
        if word_count > 0:
            avg_word_length = sum(len(word) for word in words) / word_count
            features.append(min(avg_word_length / 8.0, 1.0))
        else:
            features.append(0.0)
        
        return np.array(features).reshape(1, -1)
'''

create_file('backend/neural_network/feature_extractor.py', feature_extractor_code)

print("\nğŸ“š 5ë‹¨ê³„: ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±...")

database_code = '''"""
ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ - ëŒ€í™” ë° í”¼ë“œë°± ê´€ë¦¬
"""

import json
import os
from datetime import datetime
import numpy as np

class KnowledgeDatabase:
    def __init__(self, db_path='data/knowledge/database.json'):
        self.db_path = db_path
        self.data = self._load_database()
    
    def _load_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        if os.path.exists(self.db_path):
            try:
                with open(self.db_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"ğŸ“š ê¸°ì¡´ DB ë¡œë“œ: {len(data.get('conversations', []))}ê°œ ëŒ€í™”")
                return data
            except Exception as e:
                print(f"âš ï¸ DB ë¡œë“œ ì‹¤íŒ¨: {e}, ìƒˆë¡œ ìƒì„±")
        
        print("ğŸ“š ìƒˆë¡œìš´ ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±")
        return {'conversations': [], 'feedback': []}
    
    def save(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        with open(self.db_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
    
    def add_conversation(self, user_input, features, category, confidence, response=""):
        """ëŒ€í™” ê¸°ë¡ ì¶”ê°€"""
        conv_id = len(self.data['conversations'])
        self.data['conversations'].append({
            'id': conv_id,
            'timestamp': datetime.now().isoformat(),
            'user_input': user_input,
            'features': features.flatten().tolist(),
            'predicted_category': int(category),
            'confidence': float(confidence),
            'response': response,
            'feedback_given': False
        })
        self.save()
        return conv_id
    
    def add_feedback(self, conv_id, correct_category, rating=5):
        """ì‚¬ìš©ì í”¼ë“œë°± ì¶”ê°€"""
        if conv_id < len(self.data['conversations']):
            self.data['conversations'][conv_id]['feedback_given'] = True
            
            self.data['feedback'].append({
                'conversation_id': conv_id,
                'correct_category': int(correct_category),
                'rating': int(rating),
                'timestamp': datetime.now().isoformat()
            })
            self.save()
            return True
        return False
    
    def get_training_data(self):
        """í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±"""
        feedback_dict = {f['conversation_id']: f for f in self.data['feedback']}
        
        X_list = []
        y_list = []
        
        for conv in self.data['conversations']:
            if conv['id'] in feedback_dict:
                X_list.append(conv['features'])
                y_list.append(feedback_dict[conv['id']]['correct_category'])
        
        if len(X_list) < 3:  # ìµœì†Œ 3ê°œ í•„ìš”
            return None, None
        
        X = np.array(X_list)
        y = np.array(y_list)
        
        # One-hot encoding
        y_onehot = np.zeros((y.size, 3))
        y_onehot[np.arange(y.size), y] = 1
        
        return X, y_onehot
    
    def get_statistics(self):
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        total_conversations = len(self.data['conversations'])
        total_feedback = len(self.data['feedback'])
        
        feedback_rate = (total_feedback / max(total_conversations, 1)) * 100
        
        return {
            'total_conversations': total_conversations,
            'total_feedback': total_feedback,
            'feedback_rate': feedback_rate
        }
'''

create_file('backend/knowledge_base/database.py', database_code)

# 6ë‹¨ê³„: í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜ì •
print("\nğŸ§ª 6ë‹¨ê³„: í…ŒìŠ¤íŠ¸ íŒŒì¼ ì—…ë°ì´íŠ¸...")

test_integration_code = '''"""
ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - ê²½ë¡œ ë¬¸ì œ í•´ê²° í¬í•¨
"""

import sys
import os

# ê²½ë¡œ ë¬¸ì œ í•´ê²°
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
backend_path = os.path.join(project_root, 'backend')

print(f"ğŸ” í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
print(f"ğŸ” ë°±ì—”ë“œ ê²½ë¡œ ì¶”ê°€: {backend_path}")

# Python ê²½ë¡œì— ì¶”ê°€
if backend_path not in sys.path:
    sys.path.insert(0, backend_path)

def test_imports():
    """ëª¨ë“  ëª¨ë“ˆ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("\\nğŸ§ª ëª¨ë“ˆ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    tests = []
    
    # ì‹ ê²½ë§ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
    try:
        from neural_network.growing_network import SelfGrowingNeuralNetwork
        print("âœ… ì‹ ê²½ë§ ëª¨ë“ˆ")
        tests.append(True)
    except Exception as e:
        print(f"âŒ ì‹ ê²½ë§ ëª¨ë“ˆ: {e}")
        tests.append(False)
    
    # íŠ¹ì§• ì¶”ì¶œê¸° í…ŒìŠ¤íŠ¸
    try:
        from neural_network.feature_extractor import IRORobotFeatureExtractor
        print("âœ… íŠ¹ì§• ì¶”ì¶œê¸°")
        tests.append(True)
    except Exception as e:
        print(f"âŒ íŠ¹ì§• ì¶”ì¶œê¸°: {e}")
        tests.append(False)
    
    # ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸
    try:
        from knowledge_base.database import KnowledgeDatabase
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤")
        tests.append(True)
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤: {e}")
        tests.append(False)
    
    success_rate = sum(tests) / len(tests) * 100
    print(f"\\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {sum(tests)}/{len(tests)} ì„±ê³µ ({success_rate:.1f}%)")
    
    return all(tests)

if __name__ == "__main__":
    if test_imports():
        print("ğŸ‰ ëª¨ë“  ëª¨ë“ˆì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸ ì¼ë¶€ ëª¨ë“ˆì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
'''

create_file('tests/test_integration.py', test_integration_code)

# 7ë‹¨ê³„: ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
print("\nğŸš€ 7ë‹¨ê³„: ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")

# Python ê²½ë¡œì— ë°±ì—”ë“œ ì¶”ê°€
sys.path.insert(0, backend_path)

test_results = []

try:
    from neural_network.growing_network import SelfGrowingNeuralNetwork
    print("   âœ… ì‹ ê²½ë§ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
    test_results.append(True)
except Exception as e:
    print(f"   âŒ ì‹ ê²½ë§ ëª¨ë“ˆ ì‹¤íŒ¨: {e}")
    test_results.append(False)

try:
    from neural_network.feature_extractor import IRORobotFeatureExtractor
    print("   âœ… íŠ¹ì§• ì¶”ì¶œê¸° ë¡œë“œ ì„±ê³µ")
    test_results.append(True)
except Exception as e:
    print(f"   âŒ íŠ¹ì§• ì¶”ì¶œê¸° ì‹¤íŒ¨: {e}")
    test_results.append(False)

try:
    from knowledge_base.database import KnowledgeDatabase
    print("   âœ… ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì„±ê³µ")
    test_results.append(True)
except Exception as e:
    print(f"   âŒ ë°ì´í„°ë² ì´ìŠ¤ ì‹¤íŒ¨: {e}")
    test_results.append(False)

# ìµœì¢… ê²°ê³¼
success_count = sum(test_results)
total_count = len(test_results)

print("\n" + "=" * 60)
print(f"ğŸ“Š ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼: {success_count}/{total_count} ì„±ê³µ")
print("=" * 60)

if success_count == total_count:
    print("ğŸ‰ ëª¨ë“  ëª¨ë“ˆì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ê³  í…ŒìŠ¤íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("\\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. .env íŒŒì¼ì— OpenAI API í‚¤ ì„¤ì •")
    print("   2. python backend/main.py ì‹¤í–‰")
    print("   3. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ì ‘ì†")
else:
    print("âš ï¸ ì¼ë¶€ ëª¨ë“ˆì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ê³  í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("   pip install numpy")
