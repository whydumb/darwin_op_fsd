"""
IRO AI í•µì‹¬ ëª¨ë“ˆ ìë™ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
"""

import os

def create_file(filepath, content):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"âœ… ìƒì„±: {filepath}")

print("ğŸš€ IRO AI ëª¨ë“ˆ ìë™ ìƒì„±...")

# 1. ì‹ ê²½ë§ ëª¨ë“ˆ
growing_network_code = '''"""
ìê°€ ì„±ì¥í˜• ì‹ ê²½ë§ - ì‹¤ì œ í•™ìŠµí•˜ê³  ì„±ì¥í•˜ëŠ” AI
"""

import numpy as np
import pickle
import os
from datetime import datetime

class SelfGrowingNeuralNetwork:
    def __init__(self, input_size=10, hidden_size=8, output_size=3, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # Xavier ì´ˆê¸°í™”
        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)
        self.b2 = np.zeros((1, output_size))
        
        # í•™ìŠµ ì´ë ¥
        self.training_history = {
            'loss': [], 'accuracy': [], 'epochs': 0,
            'growth_events': [], 'total_conversations': 0
        }
        
        print(f"ğŸ§  ì‹ ê²½ë§ ì´ˆê¸°í™”: {hidden_size}ê°œ ë‰´ëŸ°")
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward(self, X):
        """ìˆœì „íŒŒ"""
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.relu(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.softmax(self.z2)
        return self.a2
    
    def backward(self, X, y_true, y_pred):
        """ì—­ì „íŒŒ - ì‹¤ì œ í•™ìŠµ"""
        m = X.shape[0]
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        dz2 = y_pred - y_true
        dW2 = np.dot(self.a1.T, dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m
        
        da1 = np.dot(dz2, self.W2.T)
        dz1 = da1 * (self.z1 > 0)
        dW1 = np.dot(X.T, dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m
        
        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
    
    def train(self, X, y, epochs=50, verbose=True):
        """ì‹ ê²½ë§ í•™ìŠµ"""
        if verbose:
            print(f"\\nğŸ“ í•™ìŠµ ì‹œì‘: {X.shape[0]}ê°œ ìƒ˜í”Œ, {epochs} ì—í¬í¬")
        
        for epoch in range(epochs):
            output = self.forward(X)
            loss = -np.sum(y * np.log(np.clip(output, 1e-15, 1-1e-15))) / X.shape[0]
            self.backward(X, y, output)
            
            accuracy = np.mean(np.argmax(output, axis=1) == np.argmax(y, axis=1))
            self.training_history['loss'].append(loss)
            self.training_history['accuracy'].append(accuracy)
            
            if verbose and (epoch + 1) % 10 == 0:
                print(f"   ì—í¬í¬ {epoch+1}/{epochs} - ì†ì‹¤: {loss:.4f}, ì •í™•ë„: {accuracy*100:.1f}%")
        
        self.training_history['epochs'] += epochs
        return accuracy
    
    def grow_network(self, new_neurons=2):
        """ì‹ ê²½ë§ í™•ì¥"""
        print(f"\\nğŸŒ± ì‹ ê²½ë§ ì„±ì¥: {self.hidden_size} â†’ {self.hidden_size + new_neurons}ê°œ ë‰´ëŸ°")
        
        old_size = self.hidden_size
        self.hidden_size += new_neurons
        
        # ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ìƒì„±
        new_W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2.0 / self.input_size)
        new_b1 = np.zeros((1, self.hidden_size))
        new_W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2.0 / self.hidden_size)
        
        # ê¸°ì¡´ ì§€ì‹ ë³´ì¡´
        new_W1[:, :old_size] = self.W1
        new_b1[:, :old_size] = self.b1
        new_W2[:old_size, :] = self.W2
        
        self.W1, self.b1, self.W2 = new_W1, new_b1, new_W2
        
        self.training_history['growth_events'].append({
            'timestamp': datetime.now().isoformat(),
            'old_size': old_size, 'new_size': self.hidden_size
        })
        
        print("âœ… ì‹ ê²½ë§ í™•ì¥ ì™„ë£Œ! ğŸ§ âœ¨")
    
    def should_grow(self, accuracy, data_count):
        """ìë™ ì„±ì¥ íŒë‹¨"""
        if self.hidden_size >= 30:
            return False, "ìµœëŒ€ í¬ê¸° ë„ë‹¬"
        if accuracy < 0.7:
            return True, f"ë‚®ì€ ì •í™•ë„ ({accuracy*100:.1f}%)"
        if data_count > 20 and self.hidden_size < 15:
            return True, f"ì¶©ë¶„í•œ ë°ì´í„° ({data_count}ê°œ)"
        return False, "í˜„ì¬ í¬ê¸°ë¡œ ì¶©ë¶„"
    
    def predict(self, X):
        return np.argmax(self.forward(X), axis=1)
    
    def get_brain_status(self):
        return {
            'neurons': self.hidden_size,
            'total_parameters': (self.input_size * self.hidden_size + 
                               self.hidden_size * self.output_size + 
                               self.hidden_size + self.output_size),
            'epochs_trained': self.training_history['epochs'],
            'growth_events': len(self.training_history['growth_events']),
            'conversations': self.training_history['total_conversations']
        }
    
    def save(self, filepath):
        """ëª¨ë¸ ì €ì¥"""
        data = {
            'weights': {'W1': self.W1, 'b1': self.b1, 'W2': self.W2, 'b2': self.b2},
            'config': {'input_size': self.input_size, 'hidden_size': self.hidden_size,
                      'output_size': self.output_size, 'learning_rate': self.learning_rate},
            'history': self.training_history
        }
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
        print(f"ğŸ’¾ ì‹ ê²½ë§ ì €ì¥: {filepath}")
    
    @classmethod
    def load(cls, filepath):
        if not os.path.exists(filepath):
            return None
        try:
            with open(filepath, 'rb') as f:
                data = pickle.load(f)
            config = data['config']
            nn = cls(**config)
            weights = data['weights']
            nn.W1, nn.b1, nn.W2, nn.b2 = weights['W1'], weights['b1'], weights['W2'], weights['b2']
            nn.training_history = data['history']
            print(f"ğŸ“‚ ì‹ ê²½ë§ ë¡œë“œ: {nn.hidden_size}ê°œ ë‰´ëŸ°")
            return nn
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
'''

# 2. íŠ¹ì§• ì¶”ì¶œê¸°
feature_extractor_code = '''"""
IRO íŠ¹í™” íŠ¹ì§• ì¶”ì¶œê¸°
"""

import numpy as np

class IRORobotFeatureExtractor:
    def __init__(self):
        self.tech_keywords = ['ì•„ë‘ì´ë…¸', 'arduino', 'ì„¼ì„œ', 'ëª¨í„°', 'ì½”ë”©', 'c++']
        self.iro_keywords = ['iro', 'ë¡œë´‡', 'ëŒ€íšŒ', 'ìš°ì£¼', 'ë¯¸ì…˜']
        self.creative_keywords = ['ì•„ì´ë””ì–´', 'ë””ìì¸', 'ì°½ì˜', 'ì„¤ê³„']
        print("ğŸ” íŠ¹ì§• ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_features(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ 10ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜"""
        if not text:
            return np.zeros((1, 10))
        
        features = []
        text_lower = text.lower()
        words = text.split()
        
        # 1-2. ê¸¸ì´ íŠ¹ì§•
        features.append(min(len(text) / 100.0, 1.0))
        features.append(min(len(words) / 30.0, 1.0))
        
        # 3. ì§ˆë¬¸ ì—¬ë¶€
        features.append(1.0 if '?' in text or 'ì–´ë–»ê²Œ' in text else 0.0)
        
        # 4-6. í‚¤ì›Œë“œ ë§¤ì¹­
        for keywords in [self.tech_keywords, self.iro_keywords, self.creative_keywords]:
            score = sum(1 for k in keywords if k in text_lower)
            features.append(min(score / max(len(keywords), 1), 1.0))
        
        # 7. ëª…ë ¹ì–´
        commands = ['í•´ì¤˜', 'ì•Œë ¤ì¤˜', 'ì„¤ëª…í•´']
        features.append(1.0 if any(c in text for c in commands) else 0.0)
        
        # 8. ê°ì • í‘œí˜„
        emotions = ['!', 'ã…‹', 'ã…', 'ì¢‹ì•„']
        features.append(min(sum(1 for e in emotions if e in text) / 3.0, 1.0))
        
        # 9. ìˆ«ì í¬í•¨
        features.append(1.0 if any(c.isdigit() for c in text) else 0.0)
        
        # 10. ë³µì¡ë„
        if words:
            avg_len = sum(len(w) for w in words) / len(words)
            features.append(min(avg_len / 8.0, 1.0))
        else:
            features.append(0.0)
        
        return np.array(features).reshape(1, -1)
'''

# 3. ë°ì´í„°ë² ì´ìŠ¤
database_code = '''"""
ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤
"""

import json
import os
from datetime import datetime
import numpy as np

class KnowledgeDatabase:
    def __init__(self, db_path='data/knowledge/database.json'):
        self.db_path = db_path
        self.data = self._load_database()
    
    def _load_database(self):
        if os.path.exists(self.db_path):
            try:
                with open(self.db_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"ğŸ“š DB ë¡œë“œ: {len(data.get('conversations', []))}ê°œ ëŒ€í™”")
                return data
            except:
                print("âš ï¸ DB ì†ìƒ, ìƒˆë¡œ ìƒì„±")
        
        print("ğŸ“š ìƒˆ DB ìƒì„±")
        return {'conversations': [], 'feedback': []}
    
    def save(self):
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        with open(self.db_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
    
    def add_conversation(self, user_input, features, category, confidence, response=""):
        conv_id = len(self.data['conversations'])
        self.data['conversations'].append({
            'id': conv_id, 'timestamp': datetime.now().isoformat(),
            'user_input': user_input, 'features': features.flatten().tolist(),
            'predicted_category': int(category), 'confidence': float(confidence),
            'response': response, 'feedback_given': False
        })
        self.save()
        return conv_id
    
    def add_feedback(self, conv_id, correct_category, rating=5):
        if conv_id < len(self.data['conversations']):
            self.data['conversations'][conv_id]['feedback_given'] = True
            self.data['feedback'].append({
                'conversation_id': conv_id, 'correct_category': int(correct_category),
                'rating': int(rating), 'timestamp': datetime.now().isoformat()
            })
            self.save()
            return True
        return False
    
    def get_training_data(self):
        feedback_dict = {f['conversation_id']: f for f in self.data['feedback']}
        X_list, y_list = [], []
        
        for conv in self.data['conversations']:
            if conv['id'] in feedback_dict:
                X_list.append(conv['features'])
                y_list.append(feedback_dict[conv['id']]['correct_category'])
        
        if len(X_list) < 3:
            return None, None
        
        X = np.array(X_list)
        y = np.array(y_list)
        y_onehot = np.zeros((y.size, 3))
        y_onehot[np.arange(y.size), y] = 1
        return X, y_onehot
    
    def get_statistics(self):
        total = len(self.data['conversations'])
        feedback = len(self.data['feedback'])
        return {
            'total_conversations': total,
            'total_feedback': feedback,
            'feedback_rate': (feedback / max(total, 1)) * 100
        }
'''

# íŒŒì¼ ìƒì„±
create_file('backend/neural_network/growing_network.py', growing_network_code)
create_file('backend/neural_network/feature_extractor.py', feature_extractor_code)
create_file('backend/knowledge_base/database.py', database_code)

# __init__.py íŒŒì¼ë“¤
init_files = [
    'backend/__init__.py',
    'backend/neural_network/__init__.py', 
    'backend/knowledge_base/__init__.py',
    'backend/api_integration/__init__.py'
]

for init_file in init_files:
    create_file(init_file, f'"""{os.path.dirname(init_file)} íŒ¨í‚¤ì§€"""\n')

print("\nğŸ‰ ëª¨ë“  ëª¨ë“ˆ ìƒì„± ì™„ë£Œ!")
print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
print("1. python tests/test_integration.py")
print("2. python backend/main.py")