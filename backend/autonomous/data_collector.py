"""
ììœ¨ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ - í•œêµ­ì–´ ìµœì í™” ë° ì•ˆì „ ëª¨ë“œ
"""

import time
import random
import re
from typing import List, Dict, Optional

# DuckDuckGo ì•ˆì „ ì„í¬íŠ¸ (ìƒˆ/êµ¬ ë²„ì „ ëª¨ë‘ ì§€ì›)
try:
    from ddgs import DDGS
    print("âœ… ìƒˆ DDGS íŒ¨í‚¤ì§€ ì‚¬ìš©")
except ImportError:
    try:
        from duckduckgo_search import DDGS
        print("âš ï¸ êµ¬ duckduckgo-search íŒ¨í‚¤ì§€ ì‚¬ìš© (ì—…ë°ì´íŠ¸ ê¶Œì¥)")
    except ImportError:
        print("âŒ DuckDuckGo ê²€ìƒ‰ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        DDGS = None

# Wikipedia ì•ˆì „ ì„í¬íŠ¸
try:
    import wikipedia
    WIKIPEDIA_AVAILABLE = True
    print("âœ… Wikipedia íŒ¨í‚¤ì§€ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    WIKIPEDIA_AVAILABLE = False
    print("âš ï¸ Wikipedia íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. DuckDuckGoë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

class AutonomousDataCollector:
    """ë…ë¦½ì  ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ - í•œêµ­ì–´ ìµœì í™”"""
    
    def __init__(self):
        self.search_engines = {}
        
        if DDGS:
            self.search_engines['duckduckgo'] = self._search_duckduckgo
        
        if WIKIPEDIA_AVAILABLE:
            self.search_engines['wikipedia'] = self._search_wikipedia
        
        self.search_history = []
    
    def autonomous_search(self, query: str, depth: str = 'moderate') -> Dict:
        """ììœ¨ì  ì •ë³´ ìˆ˜ì§‘ - í•œêµ­ì–´ íŠ¹í™”"""
        print(f"ğŸ” [ììœ¨ íƒìƒ‰] '{query}' ì¡°ì‚¬ ì‹œì‘...")
        
        collected_data = {}
        search_strategies = self._generate_search_strategies(query, depth)
        
        for strategy in search_strategies:
            engine = strategy['engine']
            search_query = strategy['query']
            
            try:
                if engine in self.search_engines:
                    results = self.search_engines[engine](search_query)
                    if results:
                        collected_data[f"{engine}_{search_query[:20]}"] = results
                        print(f"   âœ… {engine}: {len(results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                    else:
                        print(f"   âš ï¸ {engine}: ê²°ê³¼ ì—†ìŒ")
                    
                    time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                print(f"   âš ï¸ {engine} ì˜¤ë¥˜: {e}")
        
        # í•œêµ­ì–´ ìµœì í™”ëœ ë°ì´í„° ì²˜ë¦¬
        processed_data = self._process_and_evaluate_korean(collected_data, query)
        
        return {
            'query': query,
            'strategies_used': len(search_strategies),
            'raw_data': collected_data,
            'processed_data': processed_data,
            'quality_score': self._calculate_quality_score(processed_data)
        }
    
    def _generate_search_strategies(self, query: str, depth: str) -> List[Dict]:
        """í•œêµ­ì–´ ê²€ìƒ‰ ì „ëµ ìƒì„±"""
        base_strategies = [
            {'engine': 'duckduckgo', 'query': query},
        ]
        
        if WIKIPEDIA_AVAILABLE:
            base_strategies.append({'engine': 'wikipedia', 'query': query})
        
        if depth == 'deep':
            # í•œêµ­ì–´ íŠ¹í™” ê²€ìƒ‰ì–´ í™•ì¥
            additional_strategies = [
                {'engine': 'duckduckgo', 'query': f"{query} ëœ»"},
                {'engine': 'duckduckgo', 'query': f"{query} ì˜ë¯¸"},
                {'engine': 'duckduckgo', 'query': f"{query}ì´ë€"},
            ]
            base_strategies.extend(additional_strategies)
        
        return base_strategies
    
    def _search_duckduckgo(self, query: str) -> List[Dict]:
        """DuckDuckGo ê²€ìƒ‰"""
        if not DDGS:
            return []
            
        try:
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=8))
            
            return [
                {
                    'title': r.get('title', ''),
                    'content': r.get('body', ''),
                    'url': r.get('href', ''),
                    'source': 'duckduckgo'
                }
                for r in results if r.get('body', '').strip()
            ]
        except Exception as e:
            print(f"DuckDuckGo ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _search_wikipedia(self, query: str) -> List[Dict]:
        """ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰"""
        if not WIKIPEDIA_AVAILABLE:
            return []
        
        try:
            wikipedia.set_lang('ko')
            search_results = wikipedia.search(query, results=3)
            
            contents = []
            for title in search_results:
                try:
                    page = wikipedia.page(title, auto_suggest=False)
                    if page.summary.strip():
                        contents.append({
                            'title': page.title,
                            'content': page.summary,
                            'full_content': page.content[:2000],
                            'url': page.url,
                            'source': 'wikipedia'
                        })
                except Exception:
                    continue
            
            return contents
        except Exception as e:
            print(f"Wikipedia ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _process_and_evaluate_korean(self, raw_data: Dict, query: str) -> List[Dict]:
        """í•œêµ­ì–´ ìµœì í™”ëœ ë°ì´í„° ì²˜ë¦¬"""
        processed = []
        seen_titles = set()
        
        for source_key, data_list in raw_data.items():
            for item in data_list:
                if not isinstance(item, dict):
                    continue
                
                title = item.get('title', '')
                if title in seen_titles:
                    continue
                seen_titles.add(title)
                
                content = item.get('content', '') + ' ' + item.get('full_content', '')
                cleaned_content = self._clean_korean_text(content)
                
                if len(cleaned_content.strip()) < 10:  # ë„ˆë¬´ ì§§ì€ ë‚´ìš© ì œì™¸
                    continue
                
                # í•œêµ­ì–´ íŠ¹í™” ê´€ë ¨ì„± ê³„ì‚°
                relevance_score = self._calculate_korean_relevance(cleaned_content, query)
                
                print(f"   ğŸ“Š '{title[:30]}...' - ê´€ë ¨ë„: {relevance_score:.3f}")
                
                # ì„ê³„ê°’ì„ 0.1ë¡œ ëŒ€í­ ì™„í™” (í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤)
                if relevance_score > 0.1:
                    processed.append({
                        'source': item.get('source', 'unknown'),
                        'title': title,
                        'content': cleaned_content[:1200],
                        'url': item.get('url', ''),
                        'relevance_score': relevance_score,
                        'word_count': len(cleaned_content.split())
                    })
        
        processed.sort(key=lambda x: x['relevance_score'], reverse=True)
        print(f"   ğŸ“ˆ ë°ì´í„° ì •ì œ: ì›ë³¸ {len(seen_titles)}ê°œ â†’ ìœ íš¨ {len(processed)}ê°œ")
        
        return processed[:8]  # ìƒìœ„ 8ê°œ ë³´ì¡´
    
    def _clean_korean_text(self, text: str) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ì œ"""
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ë³´ì¡´
        text = re.sub(r'[^\w\sê°€-í£.,!?():-]', '', text)
        
        return text.strip()
    
    def _calculate_korean_relevance(self, content: str, query: str) -> float:
        """í•œêµ­ì–´ ìµœì í™” ê´€ë ¨ì„± ê³„ì‚°"""
        if not content or not query:
            return 0.0
            
        content_lower = content.lower()
        query_lower = query.lower()
        
        # 1. ì§ì ‘ í¬í•¨ ê²€ì‚¬ (ê°€ì¥ ë†’ì€ ì ìˆ˜)
        if query_lower in content_lower:
            # ì¿¼ë¦¬ê°€ ì œëª©ì´ë‚˜ ì²« ë¬¸ì¥ì— ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤
            first_part = content_lower[:200]
            if query_lower in first_part:
                return 0.9
            return 0.7
        
        # 2. ì¡°ì‚¬ ì œê±° í›„ ì–´ê·¼ ë§¤ì¹­
        query_stem = self._remove_korean_particles(query_lower)
        content_stems = self._remove_korean_particles(content_lower)
        
        if query_stem in content_stems:
            return 0.6
        
        # 3. ë‹¨ì–´ë³„ ë¶€ë¶„ ë§¤ì¹­
        query_words = query_lower.split()
        match_count = 0
        total_words = len(query_words)
        
        for word in query_words:
            word_stem = self._remove_korean_particles(word)
            if word in content_lower or word_stem in content_stems:
                match_count += 1
        
        if total_words > 0:
            partial_score = (match_count / total_words) * 0.5
            return partial_score
        
        return 0.0
    
    def _remove_korean_particles(self, text: str) -> str:
        """í•œêµ­ì–´ ì¡°ì‚¬ ì œê±° (ê°„ë‹¨í•œ ë²„ì „)"""
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¡°ì‚¬ë“¤ ì œê±°
        particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'í•œí…Œ', 'ê»˜']
        
        for particle in particles:
            text = text.replace(particle, '')
        
        return text
    
    def _calculate_quality_score(self, processed_data: List[Dict]) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not processed_data:
            return 0.0
        
        total_score = sum(item['relevance_score'] for item in processed_data)
        source_diversity = len(set(item['source'] for item in processed_data))
        
        return (total_score / len(processed_data)) * (1 + source_diversity * 0.1)
