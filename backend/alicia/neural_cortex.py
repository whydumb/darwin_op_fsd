"""
Aliciaì˜ ëŒ€ë‡Œí”¼ì§ˆ - ì—°ìƒ ê¸°ì–µ ë„¤íŠ¸ì›Œí¬
"""

import json
import numpy as np
from typing import Dict, List, Tuple, Optional, Set
from collections import defaultdict, deque
import os
from .compressed_neuron import CompressedNeuron

class NeuralCortex:
    """Aliciaì˜ ë‡Œ - ì••ì¶• ë‰´ëŸ° ë„¤íŠ¸ì›Œí¬ ê´€ë¦¬"""
    
    def __init__(self, storage_path: str = "data/alicia/cortex.json"):
        self.storage_path = storage_path
        self.neurons: Dict[int, CompressedNeuron] = {}
        self.concept_index: Dict[str, int] = {}  # {ê°œë…: ë‰´ëŸ°ID}
        self.topic_clusters: Dict[str, Set[int]] = defaultdict(set)
        self.next_id = 1
        
        self._ensure_directory()
        self._load_cortex()
    
    def _ensure_directory(self):
        """ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        os.makedirs(os.path.dirname(self.storage_path), exist_ok=True)
    
    def learn_concept(self, concept: str, essence: str, topic: str, 
                     related_concepts: List[str] = None) -> CompressedNeuron:
        """ìƒˆë¡œìš´ ê°œë… í•™ìŠµ ë˜ëŠ” ê¸°ì¡´ ê°œë… ê°•í™”"""
        concept_key = concept.lower().strip()
        
        if concept_key in self.concept_index:
            # ê¸°ì¡´ ê°œë… ê°•í™”
            neuron_id = self.concept_index[concept_key]
            neuron = self.neurons[neuron_id]
            neuron.activate()
            
            # ë” ë‚˜ì€ ì„¤ëª…ì´ë©´ ì—…ë°ì´íŠ¸
            if len(essence) > len(neuron.essence) * 0.8 and len(essence) < len(neuron.essence) * 1.5:
                neuron.essence = essence
            
            print(f"ğŸ§  [ê¸°ì–µ ê°•í™”] '{concept}' ê°œë…ì´ ë” ì„ ëª…í•´ì¡ŒìŠµë‹ˆë‹¤.")
            return neuron
        else:
            # ìƒˆ ë‰´ëŸ° ìƒì„±
            neuron = CompressedNeuron(
                neuron_id=self.next_id,
                concept=concept,
                essence=essence,
                topic=topic,
                knowledge_vector=self._create_knowledge_vector(essence)
            )
            
            self.neurons[self.next_id] = neuron
            self.concept_index[concept_key] = self.next_id
            self.topic_clusters[topic].add(self.next_id)
            self.next_id += 1
            
            print(f"âœ¨ [ìƒˆ ê°œë…] '{concept}' ë‰´ëŸ° ìƒì„±ë¨ (ID: {neuron.neuron_id})")
            
            # ì—°ê´€ ê°œë…ê³¼ ì—°ê²°
            if related_concepts:
                self._create_synapses(neuron.neuron_id, related_concepts)
            
            return neuron
    
    def _create_knowledge_vector(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ 64ì°¨ì› ì§€ì‹ ë²¡í„°ë¡œ ë³€í™˜"""
        words = text.lower().split()
        vector = np.zeros(64)
        
        for word in words:
            # ë‹¨ì–´ë¥¼ í•´ì‹œí•˜ì—¬ ì°¨ì›ì— ë§¤í•‘
            hash_val = hash(word) % 64
            vector[hash_val] += 1
        
        # ì •ê·œí™”
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        
        return vector
    
    def _create_synapses(self, neuron_id: int, related_concepts: List[str]):
        """ê´€ë ¨ ê°œë…ë“¤ê³¼ ì‹œëƒ…ìŠ¤ ì—°ê²°"""
        for concept in related_concepts:
            concept_key = concept.lower().strip()
            if concept_key in self.concept_index:
                target_id = self.concept_index[concept_key]
                
                # ì–‘ë°©í–¥ ì—°ê²° (ì‹œëƒ…ìŠ¤ í˜•ì„±)
                self.neurons[neuron_id].connect_to(target_id, 0.6)
                self.neurons[target_id].connect_to(neuron_id, 0.6)
                
                print(f"   ğŸ”— ì‹œëƒ…ìŠ¤ ì—°ê²°: {self.neurons[neuron_id].concept} <-> {self.neurons[target_id].concept}")
    
    def think_offline(self, query: str, max_depth: int = 2) -> Tuple[List[CompressedNeuron], List[str]]:
        """ì¸í„°ë„· ì—†ì´ ì—°ìƒ ì‚¬ê³  (í™œì„±í™” í™•ì‚°)"""
        print(f"ğŸ¤” [ì˜¤í”„ë¼ì¸ ì‚¬ê³ ] '{query}'ì— ëŒ€í•´ ìƒê° ì¤‘...")
        
        # 1. ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì‹œì‘ ë‰´ëŸ°ë“¤ ì°¾ê¸°
        start_neurons = self._find_relevant_neurons(query)
        if not start_neurons:
            return [], ["ê´€ë ¨ëœ ê¸°ì–µì´ ì—†ì–´ìš”..."]
        
        # 2. í™œì„±í™” í™•ì‚° (Spreading Activation)
        activated = set()
        thought_process = []
        queue = deque([(nid, 0) for nid, _ in start_neurons[:3]])
        
        while queue:
            neuron_id, depth = queue.popleft()
            
            if neuron_id in activated or depth > max_depth:
                continue
            
            activated.add(neuron_id)
            neuron = self.neurons[neuron_id]
            neuron.activate()
            
            thought_process.append(f"'{neuron.concept}' ë– ì˜¬ë¦¼: {neuron.essence}")
            
            # ì—°ê²°ëœ ë‰´ëŸ°ë“¤ íƒìƒ‰
            if depth < max_depth:
                sorted_synapses = sorted(neuron.synapses.items(), 
                                       key=lambda x: x[1], reverse=True)
                for connected_id, weight in sorted_synapses[:2]:  # ê°•í•œ ì—°ê²° 2ê°œ
                    if connected_id in self.neurons and weight > 0.3:
                        queue.append((connected_id, depth + 1))
                        if connected_id not in activated:
                            connected = self.neurons[connected_id]
                            thought_process.append(f"  â†’ '{connected.concept}' ì—°ìƒë¨")
        
        activated_neurons = [self.neurons[nid] for nid in activated]
        print(f"   ğŸ’¡ {len(activated_neurons)}ê°œ ê°œë… í™œì„±í™”ë¨")
        
        return activated_neurons, thought_process
    
    def _find_relevant_neurons(self, query: str) -> List[Tuple[int, float]]:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë‰´ëŸ°ë“¤ ì°¾ê¸°"""
        query_vector = self._create_knowledge_vector(query)
        scores = []
        
        for neuron_id, neuron in self.neurons.items():
            # í‚¤ì›Œë“œ ë§¤ì¹­
            keyword_score = 0.0
            query_words = set(query.lower().split())
            concept_words = set(neuron.concept.lower().split())
            essence_words = set(neuron.essence.lower().split())
            
            # Jaccard ìœ ì‚¬ë„
            if query_words & concept_words:
                keyword_score += 0.8
            if query_words & essence_words:
                keyword_score += 0.6
            
            # ë²¡í„° ìœ ì‚¬ë„
            vector_score = np.dot(query_vector, neuron.knowledge_vector)
            
            total_score = keyword_score + vector_score
            if total_score > 0.2:
                scores.append((neuron_id, total_score))
        
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores
    
    def compress_similar_neurons(self, similarity_threshold: float = 0.8):
        """ìœ ì‚¬í•œ ë‰´ëŸ°ë“¤ì„ ì••ì¶•í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½"""
        print("ğŸ—œï¸ [ë‰´ëŸ° ì••ì¶•] ìœ ì‚¬í•œ ê¸°ì–µë“¤ì„ í†µí•© ì¤‘...")
        
        compressed_count = 0
        processed = set()
        
        for neuron_id, neuron in list(self.neurons.items()):
            if neuron_id in processed:
                continue
            
            # ìœ ì‚¬í•œ ë‰´ëŸ°ë“¤ ì°¾ê¸°
            similar_ids = []
            for other_id, other in self.neurons.items():
                if other_id != neuron_id and other_id not in processed:
                    similarity = self._calculate_similarity(neuron, other)
                    if similarity >= similarity_threshold:
                        similar_ids.append(other_id)
            
            if similar_ids:
                # ìœ ì‚¬í•œ ë‰´ëŸ°ë“¤ê³¼ ë³‘í•©
                self._merge_neurons(neuron_id, similar_ids)
                processed.update(similar_ids)
                compressed_count += len(similar_ids)
        
        print(f"   âœ… {compressed_count}ê°œ ë‰´ëŸ° ì••ì¶• ì™„ë£Œ")
        self._save_cortex()
    
    def _calculate_similarity(self, neuron1: CompressedNeuron, neuron2: CompressedNeuron) -> float:
        """ë‘ ë‰´ëŸ°ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ê°œë… ìœ ì‚¬ë„
        concept_words1 = set(neuron1.concept.lower().split())
        concept_words2 = set(neuron2.concept.lower().split())
        concept_sim = len(concept_words1 & concept_words2) / len(concept_words1 | concept_words2) if concept_words1 | concept_words2 else 0
        
        # ë²¡í„° ìœ ì‚¬ë„
        vector_sim = np.dot(neuron1.knowledge_vector, neuron2.knowledge_vector)
        
        # ì£¼ì œ ìœ ì‚¬ë„
        topic_sim = 1.0 if neuron1.topic == neuron2.topic else 0.0
        
        return concept_sim * 0.5 + vector_sim * 0.3 + topic_sim * 0.2
    
    def _merge_neurons(self, main_id: int, merge_ids: List[int]):
        """ì—¬ëŸ¬ ë‰´ëŸ°ì„ ë©”ì¸ ë‰´ëŸ°ìœ¼ë¡œ ë³‘í•©"""
        main_neuron = self.neurons[main_id]
        
        for merge_id in merge_ids:
            merge_neuron = self.neurons[merge_id]
            
            # ì‹œëƒ…ìŠ¤ ì—°ê²° í†µí•©
            for syn_id, weight in merge_neuron.synapses.items():
                if syn_id in main_neuron.synapses:
                    main_neuron.synapses[syn_id] = max(main_neuron.synapses[syn_id], weight)
                else:
                    main_neuron.synapses[syn_id] = weight
            
            # í™œì„±í™” ê°•ë„ í†µí•©
            main_neuron.activation_strength = max(main_neuron.activation_strength, merge_neuron.activation_strength)
            main_neuron.source_count += merge_neuron.source_count
            
            # ë” ë‚˜ì€ ì„¤ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            if len(merge_neuron.essence) > len(main_neuron.essence):
                main_neuron.essence = merge_neuron.essence
            
            # ë³‘í•©ëœ ë‰´ëŸ° ì œê±°
            del self.neurons[merge_id]
            # ì¸ë±ìŠ¤ì—ì„œë„ ì œê±° (ê°œë…ì´ ê°™ë‹¤ë©´)
            for concept, nid in list(self.concept_index.items()):
                if nid == merge_id:
                    self.concept_index[concept] = main_id
    
    def get_cortex_stats(self) -> Dict:
        """ë‡Œ ìƒíƒœ í†µê³„"""
        total_synapses = sum(len(n.synapses) for n in self.neurons.values())
        total_memory = sum(len(str(n).encode('utf-8')) for n in self.neurons.values())
        
        return {
            'total_neurons': len(self.neurons),
            'total_synapses': total_synapses,
            'total_topics': len(self.topic_clusters),
            'memory_usage_bytes': total_memory,
            'avg_synapses_per_neuron': total_synapses / len(self.neurons) if self.neurons else 0,
            'compression_efficiency': sum(n.compression_ratio for n in self.neurons.values()) / len(self.neurons) if self.neurons else 1.0
        }
    
    def _save_cortex(self):
        """ë‡Œ ìƒíƒœ ì €ì¥"""
        data = {
            'neurons': {nid: neuron.to_dict() for nid, neuron in self.neurons.items()},
            'concept_index': self.concept_index,
            'topic_clusters': {topic: list(ids) for topic, ids in self.topic_clusters.items()},
            'next_id': self.next_id,
            'saved_at': datetime.now().isoformat()
        }
        
        with open(self.storage_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _load_cortex(self):
        """ì €ì¥ëœ ë‡Œ ìƒíƒœ ë¡œë“œ"""
        if not os.path.exists(self.storage_path):
            return
        
        try:
            with open(self.storage_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë‰´ëŸ° ë³µì›
            for nid_str, neuron_data in data.get('neurons', {}).items():
                neuron = CompressedNeuron.from_dict(neuron_data)
                self.neurons[int(nid_str)] = neuron
            
            self.concept_index = data.get('concept_index', {})
            
            # í† í”½ í´ëŸ¬ìŠ¤í„° ë³µì›
            for topic, ids in data.get('topic_clusters', {}).items():
                self.topic_clusters[topic] = set(ids)
            
            self.next_id = data.get('next_id', 1)
            
            print(f"ğŸ§  Aliciaì˜ ê¸°ì–µ ë³µì›: {len(self.neurons)}ê°œ ë‰´ëŸ°")
            
        except Exception as e:
            print(f"âš ï¸ ë‡Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
